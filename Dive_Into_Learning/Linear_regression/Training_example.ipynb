{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a simple model for predicting the values of a linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#from ipynb.fs.full.Graphic_2_Linear_regression import Synthetic_data # Own package, this one will provide a synthetic dataset\n",
    "from synthetic_data import Synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class littleTrainModel:\n",
    "    \"\"\"\n",
    "    This is a class created to train a simple model and predicting the values w, and b of a linear regresion given in\n",
    "    way of data set, each row is a line and the features are the independant variables, and the label is the \n",
    "    dependant variable.\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "            dataset_train(tuple)     = Dataset which will be our dataset of training, must be made up of two tensors, \n",
    "                                        the first one contain the features, and the second one the labels or can be\n",
    "                                        created with Graphic_2_Linear_regression module.\n",
    "            lr(float)                = learning rate\n",
    "            epochs(int)              = number of epoch\n",
    "            bacthSize(int)           = length of minibatches which will be used for training the model\n",
    "            dataSetValidation(tuple) = If isn't entered will be assigned equal to dataset_train\n",
    "            netFunc(function)        = if isn't assigned will be equal to a linear regresion\n",
    "            lossFunc(function)       = if isn't assigned will be equal to a squared error \n",
    "                                  \n",
    "    \n",
    "    Example::\n",
    "          \n",
    "        >> dataset_train  = Synthetic_data([2, -3.4],4,2, 1000).yield_data() -> dataset with parameters w[2, 3.4] and b = 4.2\n",
    "                            and 1000 examples.\n",
    "        \n",
    "        >> model = littleTrainModel(dataset_train, 0.03, 4, 10) \n",
    "        >> model.makePrediction()\n",
    "            epoch 1, loss 0.039874\n",
    "            epoch 2, loss 0.000104\n",
    "            epoch 3, loss 0.000000\n",
    "            epoch 4, loss 0.000000   \n",
    "            \n",
    "        The parameters predicted w and b are stored as attributes and can be accessed if you wish\n",
    "        \n",
    "        >> model.w \n",
    "           tensor([[ 2.0000],[-3.4000]], requires_grad=True),\n",
    " \n",
    "        >>model.b  \n",
    "           tensor([4.2000], requires_grad=True))\n",
    "           \n",
    "         Notes: \n",
    "                -In this case by the simplicity of the model and that the train and validation model are de same\n",
    "                 we have a error of zero.\n",
    "               - Because the assuption this module is obiously overfit, somethig that in the real world is for nothing \n",
    "                 desired \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataSetTrain, lr, epochs, batchSize, dataSetValidation=None, netFunc='linreg', lossFunc='squared_loss'):\n",
    "        self.w = torch.normal(0, 0.1, size=(2,1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        self.dataSetTrain = dataSetTrain\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs        \n",
    "        self.batchSize = batchSize\n",
    "        self.netFunc = netFunc\n",
    "        self.lossFunc = lossFunc\n",
    "        self.errors = []\n",
    "        \n",
    "        if dataSetValidation != None:         # If you wish put other dataset as a trining data\n",
    "            self.dataSetValidation = dataSetValidation\n",
    "        else:\n",
    "            self.dataSetValidation = dataSetTrain\n",
    "        \n",
    "    def net(self, X, w, b):                   # lineal regresion in a matriz fashion\n",
    "        if self.netFunc == 'linreg':\n",
    "            return torch.matmul(X,w) + b\n",
    "        else:\n",
    "            return self.netFunc(X,w,b)\n",
    "    \n",
    "    def loss(self,y_hat, y):                  # squared loss function\n",
    "        if self.lossFunc == 'squared_loss':\n",
    "            return ((y_hat- y.reshape(y_hat.shape))**2)/2 \n",
    "        else:\n",
    "            return self.loss(y_hat, y)\n",
    "    \n",
    "    def sgd(self, params, lr, batch_size):    # Stochastic gradient descend function\n",
    "        with torch.no_grad():             # torch.no_grad because this ones will be calculated in the training\n",
    "            for param in params:\n",
    "                param -= lr*param.grad / batch_size  \n",
    "                param.grad.zero_() \n",
    "    \n",
    "    def dataIter(self, batchSize):       #  suffle the data for more randomness \n",
    "        minibatch_features, minibatch_labels = self.dataSetValidation\n",
    "        num_examples = len(minibatch_features)   \n",
    "        indices = list(range(num_examples))  \n",
    "        random.shuffle(indices) \n",
    "        for i in range(0, num_examples, batchSize):  \n",
    "            batch_indices = indices[i: min(i + batchSize, num_examples)]  \n",
    "            yield self.dataSetValidation[0][batch_indices], self.dataSetValidation[1][batch_indices] \n",
    "       \n",
    "    def makePrediction(self):      \n",
    "        error = []\n",
    "        for epoch in range(self.epochs):   \n",
    "            for X,y in self.dataIter(self.batchSize):    # catch one minibatch and start to operate over it\n",
    "                l = self.loss(self.net(X, self.w,self.b),y) # make the loss between predicted and labels\n",
    "                # backward pass\n",
    "                l.sum().backward()   # doing the sum because backward must be perfomed over a scalar\n",
    "                # forward pass\n",
    "                self.sgd([self.w,self.b], self.lr, self.batchSize) # apply the update of  w and b  \n",
    "            with torch.no_grad(): # for each opoch calculate the loss\n",
    "                train_l = self.loss(self.net(self.dataSetValidation[0],self.w,self.b),self.dataSetValidation[1])\n",
    "                self.errors.append(float(train_l.mean()))\n",
    "                print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}') # uncomment to see the each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2094],\n",
      "        [0.0478]], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "dataset_train = Synthetic_data(true_w, true_b, 1000).yield_data()\n",
    "model = littleTrainModel(dataset_train, 0.03, 4, 10)\n",
    "print(model.w, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.035206\n",
      "epoch 2, loss 0.000077\n",
      "epoch 3, loss 0.000000\n",
      "epoch 4, loss 0.000000\n"
     ]
    }
   ],
   "source": [
    "model.makePrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** What would happen if we were to initialize the weights to zero. Would the algorithm still\n",
    "work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the funtion has the way $ Y = X*w + b$ and the backwards is calculated as the derivate of a sum, we have then that the gradient is $ \\sum_{i=0}^{n} =  \\begin{bmatrix} \\frac{\\partial Y}{\\partial w} \\\\ \\frac{\\partial Y}{\\partial w} \\end{bmatrix} = \\sum_{i=0}^{n} \\begin{bmatrix} X \\\\ 1 \\end{bmatrix} $ then the gradiend doesn't depend on the initials values w and b.\n",
    "\n",
    "And as the funtion that we are using to measure the error is a squared error and this one has a unique minima,( is practically a parabola), there is no problem with initializing in zero, because wouldn't be problem of achieve a local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0229],\n",
      "        [ 0.0869]], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "dataset_train = Synthetic_data(true_w, true_b, 1000).yield_data()\n",
    "model = littleTrainModel(dataset_train, 0.03, 4, 10)\n",
    "print(model.w, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.047506\n",
      "epoch 2, loss 0.000144\n",
      "epoch 3, loss 0.000000\n",
      "epoch 4, loss 0.000000\n"
     ]
    }
   ],
   "source": [
    "model.makePrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** why does the following torch.matmul operation raise the **RuntimeError: expected scalar type Long but found Float** error? \n",
    "\n",
    "<code>true_w = torch.tensor([2, -3.])\n",
    "true_b = 4\n",
    "features = torch.tensor([[5,4],[3,2]])\n",
    "labels = torch.matmul(features,true_w) + true_b\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is because Pytorch can't perfom broadcasting operations over a different Data types for example in the above snippet the value true_w has assigned a torch.float32 while the features has torch.int64\n",
    "\n",
    "<code>\\>> print(true_w.dtype)\n",
    "\\>> torch.float32\n",
    "\n",
    "\\>> print(features.dtype)\n",
    "\\>> torch.int64</code>\n",
    "\n",
    "If in the previous code the -3.0 is change by a -3, then the equation can be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3__ Why is the reshape function needed in the squared_loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "<code>\n",
    " def loss(self,y_hat, y):            \n",
    "        if self.lossFunc == 'squared_loss':\n",
    "            return ((y_hat- y.reshape(y_hat.shape))**2)/2 \n",
    "        else:\n",
    "            return self.loss(y_hat, y)</code>\n",
    "            \n",
    "### Loss function in the epochs\n",
    "<code>\n",
    "for epoch in range(self.epochs):       \n",
    "    for X,y in self.dataIter(self.batchSize): \n",
    "        l = self.loss(self.net(X, self.w,self.b),y) \n",
    "</code>\n",
    "\n",
    "### net function that is the first input of the Loss function\n",
    "\n",
    "<code>def net(self, X, w, b):     \n",
    "        if self.netFunc == 'linreg':\n",
    "          return torch.matmul(X,w) + b\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The caller of the loss function requiere two inputs, the first one the net that perform the prediction, and the second one the labels (true values), as the net return the result of a multiplication this one has one shape of \\[num_examples\\] and the __y__ (labels) has one shape of [numexample, 1], the followin example, show the shapes of the y_hat and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([2, -3.])\n",
    "b = 4\n",
    "data =  Synthetic_data(w,b, 10).yield_data()\n",
    "X = data[0]\n",
    "y_hat = data[1]\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, w, b):\n",
    "        return torch.matmul(X,w) + b\n",
    "y = net(X,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we perform y_hat - y we get a broadcasting of every y_hat's row with every column, and then finally we would get a tensor that is not the desired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.__ Experiment using different learning rates to find out how fast the loss function value drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.1, 0.08, 0.06, 0.04, 0.02, 0.001,0.0001]\n",
    "w = torch.tensor([2, -3.4])\n",
    "b = 4.2\n",
    "num_examples = 1000\n",
    "epochs = 3\n",
    "\n",
    "class ErrorDependOnLr:\n",
    "    \n",
    "    def __init__(self, lrs, w, b, num_examples, epochs):\n",
    "          \n",
    "        self.lrs = lrs\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.num_examples = num_examples\n",
    "        self.epochs = epochs\n",
    "        self.errors = []\n",
    "        \n",
    "    def EpochError(self):\n",
    "        errors = []\n",
    "        dataset_train = Synthetic_data(self.w , self.b, self.num_examples).yield_data()\n",
    "\n",
    "        for lr in self.lrs: \n",
    "            model = littleTrainModel(dataset_train, lr, self.epochs, 10)\n",
    "            model.makePrediction()\n",
    "            self.errors.append(model.errors[self.epochs-1])\n",
    "        \n",
    "    def graph(self):\n",
    "        self.EpochError()\n",
    "        ax = plt.axes()\n",
    "        plt.title('Errors depend on Learning rate')\n",
    "        plt.grid(True)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('Error')\n",
    "        ax.axis([0.8, 6.2, -0.5, 10])\n",
    "        labels = self.lrs\n",
    "        \n",
    "        for point in enumerate(self.errors, start=1):\n",
    "            ax.scatter(point[0],point[1])\n",
    "            ax.legend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = ErrorDependOnLr(lrs, w, b, num_examples, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000000\n",
      "epoch 2, loss 0.000000\n",
      "epoch 3, loss 0.000000\n",
      "epoch 1, loss 0.000001\n",
      "epoch 2, loss 0.000000\n",
      "epoch 3, loss 0.000000\n",
      "epoch 1, loss 0.000088\n",
      "epoch 2, loss 0.000000\n",
      "epoch 3, loss 0.000000\n",
      "epoch 1, loss 0.005282\n",
      "epoch 2, loss 0.000002\n",
      "epoch 3, loss 0.000000\n",
      "epoch 1, loss 0.297496\n",
      "epoch 2, loss 0.005735\n",
      "epoch 3, loss 0.000113\n",
      "epoch 1, loss 13.687961\n",
      "epoch 2, loss 11.252166\n",
      "epoch 3, loss 9.250182\n",
      "epoch 1, loss 16.321453\n",
      "epoch 2, loss 16.004808\n",
      "epoch 3, loss 15.694309\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkY0lEQVR4nO3de3yU5Z338c+PHAgSDAKCSEDQbBFQy1FLqRawFYsV213W0q6K9qDdQotYj9Uq9dWn6vag7tKnlae21a2aVTxEalewQCq2FkToykJUVDwkgCAWTJAEEn7PHzOJk5CQyUwyN8n1fb9e83Lmuk+/a4jf3Lnue64xd0dERMLRLeoCREQksxT8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfBLJMxsqJm5mWVHXUsiM1tgZr+Luo5MMbONZjY56joksxT8XYiZvWlm+8ysKuGxMOq65FBHyi8Ydx/l7qVR1wENP7+fibqOEBxRZ1vSLs539z+2tpKZZbt7bZO2LHevS/ZAbV1fMqu5f+OoHEm1iM74g2Fml5rZn83sTjPbBSwws9+a2S/M7A9mtheYYmYjzKzUzHbHhwFmJOyjufWnm9kmM6s0swozu7qF42eZ2U/M7D0zewM4r8nyAjO718y2xffzQzPLalL7QjPbY2Yvm9nZbdj2ufix/25mW8zscwnbDjOzP8Xrfwbo18r7+A0ze83M3jezJ83s+IRlbmbfNLPN8ffv52Zmbfhnqt/PJ8zsL/F9/E/iUIyZXWZmZfF63zCzKxKWTTazcjO7zsy2A7+J/2XxsJndH99mo5mNT9im4Sw7iXXHmtn6+LJHzOy/zOyHLfShuZ+3k8xshZntiv8cPGBmvePr/ycwBFhisb9Ur23tvZA0uLseXeQBvAl8poVllwK1wLeJ/aXXA/gtsAeYROwkoBfwGvA9IBeYClQCw+P7aLp+HrANODO+/BhgbAvH/ybwMjAY6AOsBBzIji9/HLgH6An0B9YAVzSpfT6QA3wpXkefJLc9AHwDyAL+FdgKWHz588DPgO7AWfH+/q6FPkwF3gPGxtf/D+DZhOUO/B7oTSzEdgLntrCvBc0dBxgE7AKmx9/jz8ZfHxtffh5wEmDAp4EP699zYHL8fbojXl+P+HGq4/vLAm4D/trcz8zh1o3/PLwFzIv/G/wjsB/4YRt+3ori/ekOHAs8C9zV0s9va++FHmlkRdQF6NGO/5ix/3GqgN0Jj2/El10KvN1k/d8C9ye8PhPYDnRLaHsIWNDc+vG2t4ErgKNbqW0F8M2E1+fEgzIbGADUAD0Sln8ZWJlQe0NYx9vWABcnue1rCcuOih/3OGLhXAv0TFj+IC0H/73AvyW8zif2S2Vo/LUDn0pY/jBwfQv7WtDccYDrgP9s0rYUmN3Cfp4A5sWfTyYWxnlNjvPHhNcjgX1NfmY+09q6xH4pVjT5N3iOwwf/280tS1jnC8D65mpJ5b3QI/mHxvi7ni94y2P877TSdjzwjrsfTGh7i9iZV0v7+CfgJuB2M3uJWNA938xxjm+y7VsJz08gdha5LWFkpFuT9Ss8/n9+wvbHJ7nt9von7v5hfL18YsM6f3f3vU32O7iZ+uv7sC5hX1XxYYxBxEKr0bGInY3nt7CvlpwA/LOZnZ/QlkPsLyTiw1S3AB8j1s+jgA0J6+509+om+2xaU561PObe7LrE+t7036C5nydaWm5mA4C7iZ1g9IrX//fDbH/Y90JSp+APS3NTsSa2bQUGm1m3hPAfArza0j7c/QXgAjPLAeYSO8ttLji3NWkfkvD8HWJn7f1aCCOAQWZmCcEzBHgyyW1bsg04xsx6JoT/EJp/nyD2/pxQ/8LMegJ9iZ0Jt5d3iJ3lfqPpAjPrDjwKXAKUuPsBM3uC2LBPvY6abncbh/4bDAZeP8w2TWv5UbztVHd/38y+ACw8zPotvheSHl3clUSriZ3lXWtmOfELaecDxc2tbGa5ZvYvZlbg7geAD4CDza1L7BfCd8ys0MyOAa6vX+Du24BlwE/N7Ggz6xa/EPjphO37x7fPMbN/BkYAf0hy22a5+1vAWuAH8b58Kt7fljwEXGZmo+Mh/CNgtbu/2dqxWtDNzPISHt2B3wHnm9k0i10Qz4tftC0kNs7endi1g9r42f85KR67rZ4H6oC5ZpZtZhcAp7dxH72IDUXuMbNBwDVNlr8LnJjw+nDvhaRBwd/11N8VUf94PNkN3X0/seD7HLGLmP8XuMTdXz7MZhcDb5rZB8Qu4P5LC+v9P2Ljs/9DbLjksSbLLyEWbJuI/fm/GBiYsHw18A/xuv4PMNPddyW57eF8BTgDeJ/YEMr9La0YH0L7PrGz7m3ELrLOSvI4zfkysC/h8bq7vwNcQOwC+05iZ73XELvuUgl8h9gv0b/Ha38yjeMnLf6z8Y/A14hdO7qI2IXsmjbs5gfELozvAZ7i0J+B24Cb4nfwXH249yL1ngh8dGeDyBHLzC4Fvu7un4q6FvmIma0Gfunuv4m6Fmkb/eYUkaSY2afN7Lj4UM9s4DTg6ajrkrbrsOA3s1+b2Q4z+9+Etj5m9ozFPuDyTHysV0Q6h+HEhup2A98lNty2LdKKJCUdNtRjZmcRu5Bzv7ufEm/7N+B9d7/dzK4HjnH36zqkABERaVaHjvGb2VDg9wnB/wow2d23mdlAoNTdh3dYASIicohM38c/IOFPw+3EPnXZLDO7HLgcoEePHuMGD27pMzWHOnjwIN26hXf5Qv0Oi/odllT6/eqrr77n7sc2bY/sA1zu7mbW4p8b7r4IWAQwfvx4X7t2bdL7Li0tZfLkyWnX2Nmo32FRv8OSSr/N7K3m2jP9a/Pd+BAP8f/uyPDxRUSCl+ngfxKYHX8+GyjJ8PFFRILXkbdzPkTsY97DLTZH+NeA24HPmtlm4DPx1yIikkEdNsbv7l9uYdHZLbS3yYEDBygvL6e6uulEhFBQUEBZWVl7HKZTyc/P58CBA+Tk5ERdiogcwTrt7Jzl5eX06tWLoUOHYk2+5KiyspJevXpFVFk03J3y8nLKy8sZNmxY1OWIyBGs094TVV1dTd++fQ8J/VCZGQUFBc3+BSQikqjTBj+g0G9C74eIJKNTB7+IiLSdgj8NTz/9NMOHD6eoqIjbbz/0BqVnn32WsWPHkp2dzeLFiyOoUETkUAr+FNXV1TFnzhz++7//m02bNvHQQw+xadOmRusMGTKE3/72t3zlK1+JqEoRkUN12rt62uqJ9RX8eOkrbN29j+N79+CaacP5wphBrW/YgjVr1lBUVMSJJ8a+KW7WrFmUlJQwcuTIhnWGDh0KEOS8IiJy5AoikZ5YX8ENj22gYvc+HKjYvY8bHtvAE+tT/47siooKEieOKywspKKiPb9zW0QEylatZNGcy3j3jddYNOcyylatTHufQQT/j5e+wr4DdY3a9h2o48dLX4moIhGR1pWtWsmyRQupfG8nAJXv7WTZooVph38Qwb919742tSdj0KBBvPPOOw2vy8vLGTQo9aEjEZGmVhXfT+3+xt9nX7u/hlXF96e13yCC//jePdrUnowJEyawefNmtmzZwv79+ykuLmbGjBkp709EpKnKXe+1qT1ZQQT/NdOG0yMnq1Fbj5wsrpmW+pd/ZWdns3DhQqZNm8aIESO48MILGTVqFDfffDNPPvkkAC+88AKFhYU88sgjXHHFFYwaNSqtfohIWHr17dem9mQFcVdP/d077XlXD8D06dOZPn16o7Zbb7214fmECRMoLy9P6xgiEq4zZ13CskULGw33ZOd258xZl6S13yCCH2Lhn27Qi4hk0ogzpwA0jOn36ncsZ866pKE9VcEEv4hIZzTizCmMOHMKpaWlfOmrX2+XfQYxxi8iIh9R8IuIBEbBLyISGAW/iEhgFPxpaG1a5pqaGr70pS9RVFTEGWecwZtvvgnEvi949uzZnHrqqYwYMYLbbrstw5WLSMgU/ClKZlrme++9l2OOOYbXXnuN+fPnc9111wHwyCOPUFNTw4YNG3jxxRe55557Gn4piIh0tHCC/6WH4c5TYEHv2H9fejit3SVOy5ybm9swLXOikpISZs+eDcDMmTNZvnw57o6ZsXfvXmpra9m3bx+5ubkcffTRadUjIpKsMIL/pYdhyXdgzzuAx/675DtphX8y0zInrpOdnU1BQQG7du1i5syZ9OzZk4EDBzJkyBCuvvpq+vTpk3ItIiJtEUbwL78VDjSZifPAvlh7BNasWUNWVhZbt25ly5Yt/PSnP+WNN96IpBYRCU8Ywb+nhflyWmpPQjLTMieuU1tby549e+jbty8PPvgg5557Ljk5OfTv359Jkyaxdu3alGsREWmLMIK/oLBt7UlIZlrmGTNmcN999wGwePFipk6dipkxZMgQVqxYAcDevXv561//ysknn5xyLSIibRFG8J99M+Q0mXs/p0esPUXJTMv8ta99jV27dlFUVMTPfvazhls+58yZQ1VVFaNGjWLChAlcdtllnHbaaSnXIiLSFmFM0nbahbH/Lr81NrxTUBgL/fr2FLU2LXNeXh6PPPLIIdvl5+c32y4ikglhBD/EQj7NoBcR6QrCGOoREZEGCn4RkcAo+EVEAqPgFxEJTCTBb2bzzWyjmf2vmT1kZnlR1CEiEqKMB7+ZDQK+A4x391OALGBWputoD6lOywzw0ksvMXHiREaNGsWpp55KdXV1BisXkZBFNdSTDfQws2zgKGBrRHWkLJ1pmWtra7nooov45S9/ycaNGyktLSUnJyeKbohIgDJ+H7+7V5jZT4C3gX3AMndf1nQ9M7scuBxgwIABlJaWNlpeUFBAZWVls8eoq6s7ZNnSt5fyy42/ZMe+HfTv0Z9vjvom04ZMS7kfq1evZujQoRx77LHU1NTwxS9+kYcffpjvfve7Des8+uij3HDDDVRWVjJt2jTmzJnDBx98wLJlyxgxYgQnnngilZWV5Obm8uGHH6ZcS726ujqqq6sPea+6uqqqquD6DOp3aNqz3xkPfjM7BrgAGAbsBh4xs4vc/XeJ67n7ImARwPjx433y5MmN9lNWVkavXr2aPUZlZWWjZU+98RR3rL+D6rrYcMq7+97ljvV30KNHD8478byU+rF7926GDRvWcJyTTjqJ1atXNzruu+++y8knn9zQ1rt3b/bv3095eTm5ubnMnDmTnTt3MmvWLK699tqU6khUWVlJXl4eY8aMSXtfnUlpaSlNfz5CoH6HpT37HcVQz2eALe6+090PAI8Bn+zIA9697u6G0K9XXVfN3evu7sjDtqi2tpbnnnuOBx54gOeee47HH3+c5cuXR1KLiIQniuB/G/iEmR1lZgacDZR15AG3793epvZkpDMtc2FhIWeddRb9+vXjqKOOYvr06axbty7lWkRE2iLjwe/uq4HFwDpgQ7yGRR15zON6Htem9mSkMy3ztGnT2LBhAx9++CG1tbX86U9/YuTIkSnXIiLSFpHc1ePut7j7ye5+irtf7O41HXm8eWPnkZfV+KMCeVl5zBs7L+V9pjMt8zHHHMNVV13FhAkTGD16NGPHjuW881K71iAi0lZBzM5ZfwH37nV3s33vdo7reRzzxs5L+cJuvVSnZQa46KKLuOiii9I6vohIKoIIfoiFf7pBLyLSFWiuHhGRwCj4RUQCo+AXEQmMgl9EJDAKfhGRwCj405DOtMwAb7/9Nvn5+fzkJz/JUMUiIgr+lKUzLXO9q666is997nOZLFtEJJzg37NkCZunnk3ZiJFsnno2e5YsSWt/a9asoaioiBNPPJHc3FxmzZpFSUlJo3VKSkqYPXs2ADNnzmT58uW4OwBPPPEEw4YNY9SoUWnVISLSVkEE/54lS9j2/Zup3boV3KndupVt3785rfCvqKhg8ODBDa8LCwupqKhocZ3s7GwKCgrYtWsXVVVV3HHHHdxyyy0pH19EJFVBBP+OO+/Cm3y1oVdXs+POuyKpZ8GCBcyfP5/8/PxIji8iYQtiyobabdva1J6MtkzLXFhY2Gha5tWrV7N48WKuvfZadu/eTbdu3cjLy2Pu3Lkp1yMikqwggj974MDYME8z7alKnJZ50KBBFBcX8+CDDzZap35a5okTJzaalnnVqlUN6yxYsID8/HyFvohkTBBDPf3nX4nlNZ6W2fLy6D//ypT3mc60zCIiUQrijL/g/POB2Fh/7bZtZA8cSP/5Vza0pyqdaZnrLViwIK0aRETaKojgh1j4pxv0IiJdQRBDPSIi8hEFv4hIYBT8IiKBUfCLiARGwS8iEhgFfxpSnZb5mWeeYdy4cZx66qmMGzeOFStWZLhyEQmZgj9F6UzL3K9fP5YsWcKGDRu47777uPjii6PogogEKpj7+F9dvZ3nS16n6v0a8vt0Z+IFJ/GxM45LeX+J0zIDDdMyjxw5smGdkpKShg9ozZw5k7lz5+LujBkzpmGdUaNGsW/fPmpqaujevXvK9YiIJCuIM/5XV29n5QMvU/V+DQBV79ew8oGXeXX19pT3mc60zIkeffRRxo4dq9AXkYwJIvifL3md2v0HG7XV7j/I8yWvR1RRzMaNG7nuuuu45557Iq1DRMISRPDXn+kn256MtkzLDDSalrl+/S9+8Yvcf//9nHTSSSnXISLSVkEEf36f5odRWmpPRuK0zPv376e4uJgZM2Y0Wqd+Wmag0bTMu3fv5rzzzuP2229n0qRJKdcgIpKKIIJ/4gUnkZ3buKvZud2YeEHqZ9rpTMu8cOFCXnvtNW699VZGjx7N6NGj2bFjR+odFBFpgyDu6qm/e6c97+qB1Kdlvummm7jpppvSOraISKqCCH6IhX+6QS8i0hVEMtRjZr3NbLGZvWxmZWY2MYo6RERCFNUZ/93A0+4+08xygaMiqkNEJDgZD34zKwDOAi4FcPf9wP5M1yEiEipz98we0Gw0sAjYBHwceBGY5+57m6x3OXA5wIABA8YVFxc32k9BQQFFRUXNHqOuro6srKx2r/1IV1dXx5YtW9izZ0/UpWRUVVUV+fn5UZeRcep3WFLp95QpU1509/FN26MI/vHAX4FJ7r7azO4GPnD377e0zfjx433t2rWN2srKyhgxYkSz61dWVtKrV692rLpzqKyspLy8vMX3pasqLS1l8uTJUZeRcep3WFLpt5k1G/xRXNwtB8rdfXX89WJgbAR1pC3VaZkBbrvtNoqKihg+fDhLly5taP/qV79K//79OeWUUzLRBREJUMaD3923A++Y2fB409nEhn06lXSmZd60aRPFxcVs3LiRp59+mm9961vU1dUBcOmll/L0009nvD8iEo6oPrn7beABM3sJGA38qKMPWLZqJYvmXMZPZ53PojmXUbZqZVr7S5yWOTc3t2Fa5kQlJSXMnj0biE3LvHz5ctydkpISZs2aRffu3Rk2bBhFRUWsWbMGgLPOOos+ffqkVZuIyOFEcjunu/8NOGTcqaOUrVrJskULqd0fm5St8r2dLFu0EIARZ05JaZ/NTcu8evXqFtdJnJa5oqKCT3ziE422bTqls4hIRwlirp5Vxfc3hH692v01rCq+P6KKRESiE0TwV+56r03tyUhnWuZkthUR6ShBBH+vvv3a1J6MdKZlnjFjBsXFxdTU1LBlyxY2b97M6aefnnItIiJtEUTwnznrErJzG8+9n53bnTNnXZLyPtOZlnnUqFFceOGFjBw5knPPPZef//znDR84+/KXv8zEiRN55ZVXKCws5N577025RhGR5gQxO2f9BdxVxfdTues9evXtx5mzLkn5wm69VKdlBrjxxhu58cYbD2l/6KGH0qpJRKQ1QQQ/xMI/3aAXEekKghjqERGRjyj4RUQCo+AXEQmMgl9EJDCtBr+ZdTOzT2aiGBER6XitBr+7HwR+noFaOp2OmJa5pX0uXLiQoqIizIz33kv9E8ciIskO9Sw3s38yM+vQajqRjpiW+XD7nDRpEn/84x854YQTMt5XEelakr2P/wrgKqDOzPYBBri7H91hlbWzvet38MHSN6nbXUNW7+4cPW0oPcf0T3l/idMyAw3TMo8cObJhnZKSEhYsWADEpmWeO3duq9Myt7TPMWPGpFyriEiipM743b2Xu3dz9xx3Pzr+ulOF/u7HNlO3OzZDZ93uGnY/tpm963ekvM/mpmVuOrXy4aZlbm7bZPYpIpKupD+5a2YzgLPiL0vd/fcdU1L7+2Dpm/iBg43a/MBBPlj6Zlpn/SIinVFSZ/xmdjswj9hXJG4C5pnZbR1ZWHuqP9NPtj0ZHTEts6ZrFpFMSPbi7nTgs+7+a3f/NXAucF7HldW+snp3b1N7MjpiWuZk9ikikq62fICrd8Lzgnauo0MdPW0oltO4q5bTjaOnDU15nx0xLXNL+wT493//dwoLCykvL+e0007j61//esq1i0jYzN1bX8lsFnAHsJLYHT1nAde7+391bHkx48eP97Vr1zZqKysrY8SIEc2uX1lZSa9evRq1tfddPUeiyspKysvLW3xfuqrS0lImT54cdRkZp36HJZV+m9mL7n7I95u3enHXzLoBB4FPABPizde5+/Y2VRCxnmP6d7mgFxFJRavB7+4Hzexad38YeDIDNYmISAdKdoz/j2Z2tZkNNrM+9Y8OrSwJyQxThUTvh4gkI9n7+L8U/++chDYHTmzfcpKXl5fHrl276Nu3L5pJIhb6e/bsIS8vL+pSROQIl+wYf8Yu5Car/g6XnTt3HrKsuro6yADcu3cvH//4x6MuQ0SOcMmO8V8DHFHBn5OTw7Bhw5pdVlpaGuTcNqWlpeTk5ERdhogc4Tr1GL+IiLRdpx3jFxGR1CQV/O7e/JiKiIh0Oocd6jGzaxOe/3OTZT/qqKJERKTjtDbGPyvh+Q1Nlp3bzrWIiEgGtBb81sLz5l6LiEgn0FrwewvPm3vdJmaWZWbrzazTfKGLiEhX0NrF3Y+b2QfEzu57xJ8Tf53uJ6TmAWVAp/kKRxGRruCwZ/zunpXwHbvZ8ef1r1P+pJCZFRL7IpdfpboPERFJTVu+iKU93QVcS2y6ZxERyaCkvoilXQ9o9nlgurt/y8wmA1e7++ebWe9y4HKAAQMGjCsuLk76GFVVVeTn57dPwZ2I+h0W9TssqfR7ypQpzX4RC+6e0QdwG1AOvAlsBz4Efne4bcaNG+dtsXLlyjat31Wo32FRv8OSSr+Btd5MpmZ8qMfdb3D3QncfSuxzAivc/aJM1yEiEqqoxvhFRCQiyU7S1iHcvRQojbIGEZHQ6IxfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwCn4RkcAo+EVEAqPgFxEJjIJfRCQwGQ9+MxtsZivNbJOZbTSzeZmuQUQkZNkRHLMW+K67rzOzXsCLZvaMu2+KoBYRkeBk/Izf3be5+7r480qgDBiU6TpEREJl7h7dwc2GAs8Cp7j7B02WXQ5cDjBgwIBxxcXFSe+3qqqK/Pz8dqy0c1C/w6J+hyWVfk+ZMuVFdx9/yAJ3j+QB5AMvAv/Y2rrjxo3ztli5cmWb1u8q1O+wqN9hSaXfwFpvJlMjuavHzHKAR4EH3P2xKGoQEQlVFHf1GHAvUObuP8v08UVEQhfFGf8k4GJgqpn9Lf6YHkEdIiJByvjtnO7+HGCZPq6IiMTok7siIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgFPwiIoFR8IuIBEbBLyISGAW/iEhgIgl+MzvXzF4xs9fM7Pr22u8T6yuYdPsKNlTsYdLtK3hifUV77frI9tLDcOcpsO1vsf++9HDUFWXEU288xTmLz2HTrk2cs/gcnnrjqahLyog9S5aweerZVG/cyOapZ7NnyZKoS8qIV1dv577v/Zmdb1dy3/f+zKurt0ddUqeVnekDmlkW8HPgs0A58IKZPenum9LZ7xPrK7jhsQ3sO1AHg6Fi9z5ueGwDAF8YMyjtuo9YLz0MS74DB/bBccCed2KvAU67MNLSOtJTbzzFgr8soLquGvJh295tLPjLAgDOO/G8aIvrQHuWLGHb92/Gq6sBqN26lW3fvxmAgvPPj7K0DvXq6u2sfOBlavcf5Cig6v0aVj7wMgAfO+O4aIvrhKI44z8deM3d33D3/UAxcEG6O/3x0ldioZ9g34E6frz0lXR3fWRbfmss9BMd2Bdr78LuXnd3LPQTVNdVc/e6uyOqKDN23HlXQ+jX8+pqdtx5VzQFZcjzJa9Tu/9go7ba/Qd5vuT1iCrq3MzdM3tAs5nAue7+9fjri4Ez3H1uk/UuBy4HGDBgwLji4uLD7ndDxZ6G5wN6wLsJWXjqoIJ2qv4ItO1vDU+ruh9Pfs3Wj5YNHJ3xcjJl066P/kA8NutYdtbtbHg9su/IKErKiOqNGxue7+/fn9wdOxpe540aFUVJGbHz7cqG59k9D1K796Nz1mOH9IqipIyrqqoiPz+/TdtMmTLlRXcf37T9iA3+ROPHj/e1a9cedr+Tbl9Bxe5Y2n/31Fp+uiE2ijWodw/+fP3Udqr+CHTnKbHhHaB0+A+Y/MotsfaCwTD/fyMsrGOds/gctu3dBsC/5v8rv6j6BQADew5k2cxlUZbWoTZPPZvarbFf7m99ey4n/MdCALKPP55/WLE8ytI61H3f+zNV79cA0P+Te9nxl54A5PfpzuwfTYqytIwpLS1l8uTJbdrGzJoN/iiGeiqAwQmvC+Ntablm2nB65GQ1auuRk8U104anu+sj29k3Q06Pxm05PWLtXdi8sfPIy8pr1JaXlce8sfMiqigz+s+/Estr3G/Ly6P//CujKShDJl5wEtm5jeMqO7cbEy84KaKKOreMX9wFXgD+wcyGEQv8WcBX0t1p/QXc2Jh+JYN69+CaacO79oVd+OgCbv2YfsHgWOh34Qu78NEF3Pox/YE9BzJv7LwufWEXPrqAWz+mn3388fSff2WXvrALH13AjY3p7yW/T3cmXnCSLuymyt0z/gCmA68CrwM3trb+uHHjvC1WrlzZpvW7CvU7LOp3WFLpN7DWm8nUKM74cfc/AH+I4tgiIqHTJ3dFRAKj4BcRCYyCX0QkMAp+EZHAKPhFRAKT8U/upsLMdgJvtWGTfsB7HVTOkUz9Dov6HZZU+n2Cux/btLFTBH9bmdlab+Zjyl2d+h0W9Tss7dlvDfWIiARGwS8iEpiuGvyLoi4gIup3WNTvsLRbv7vkGL+IiLSsq57xi4hICxT8IiKB6VLBb2a/NrMdZtZ1v3qqGWY22MxWmtkmM9toZl3720jizCzPzNaY2f/E+/2DqGvKJDPLMrP1Zvb7qGvJFDN708w2mNnfzOzwX8vXhZhZbzNbbGYvm1mZmU1Ma39daYzfzM4CqoD73f2UqOvJFDMbCAx093Vm1gt4EfiCu29qZdNOzcwM6OnuVWaWAzwHzHP3v0ZcWkaY2VXAeOBod/981PVkgpm9CYx396A+wGVm9wGr3P1XZpYLHOXuu1PdX5c643f3Z4H3o64j09x9m7uviz+vBMqALv7VYxD/romq+Muc+KPrnMkchpkVAucBv4q6FulYZlYAnAXcC+Du+9MJfehiwS9gZkOBMcDqiEvJiPhwx9+AHcAz7h5Ev4G7gGuBgxHXkWkOLDOzF83s8qiLyZBhwE7gN/GhvV+ZWc90dqjg70LMLB94FLjS3T+Iup5McPc6dx8NFAKnm1mXH+Izs88DO9z9xahricCn3H0s8DlgTnx4t6vLBsYCv3D3McBe4Pp0dqjg7yLiY9yPAg+4+2NR15Np8T99VwLnRlxKJkwCZsTHu4uBqWb2u2hLygx3r4j/dwfwOHB6tBVlRDlQnvDX7GJivwhSpuDvAuIXOe8Fytz9Z1HXkylmdqyZ9Y4/7wF8Fng50qIywN1vcPdCdx8KzAJWuPtFEZfV4cysZ/zmBeJDHecAXf4OPnffDrxjZsPjTWcDad24EcmXrXcUM3sImAz0M7Ny4BZ3vzfaqjJiEnAxsCE+3g3wvfiX2ndlA4H7zCyL2EnMw+4ezK2NARoAPB47zyEbeNDdn462pIz5NvBA/I6eN4DL0tlZl7qdU0REWqehHhGRwCj4RUQCo+AXEQmMgl9EJDAKfhGRwCj4RUQCo+AXEQmMgl8kBWY2wcxein8nQM/49wF0+XmCpGvQB7hEUmRmPwTygB7E5lK5LeKSRJKi4BdJUfzj8y8A1cAn3b0u4pJEkqKhHpHU9QXygV7EzvxFOgWd8YukyMyeJDYt8jBiX305N+KSRJLSpWbnFMkUM7sEOODuD8ZnB/2LmU119xVR1ybSGp3xi4gERmP8IiKBUfCLiARGwS8iEhgFv4hIYBT8IiKBUfCLiARGwS8iEpj/D3Cx0R8fo8CSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.692992413950214e-13,\n",
       " 5.142055401266554e-13,\n",
       " 1.9405490669227587e-12,\n",
       " 7.494685982933902e-10,\n",
       " 0.00011285708023933694,\n",
       " 9.250182151794434,\n",
       " 15.69430923461914]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
